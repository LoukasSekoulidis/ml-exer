{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2+YWmbXLzjzFOD+Yfw3bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoukasSekoulidis/ml-exer/blob/main/sentiment-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AdamW, BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "data = load_dataset('dair-ai/emotion')\n",
        "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Load data\n",
        "train_data = data['train']\n",
        "validation_data = data['validation']\n",
        "test_data = data['test']\n",
        "\n",
        "# Tokenize data\n",
        "def tokenization(data):\n",
        "    return tokenizer(data['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_data = train_data.map(tokenization, batched=True)\n",
        "validation_data = validation_data.map(tokenization, batched=True)\n",
        "test_data = test_data.map(tokenization, batched=True)\n",
        "\n",
        "# Format data\n",
        "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Set batch size and data loader\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06N3MJwCaYj5",
        "outputId": "e8bf10b4-4dae-47f3-eb91-6c4428e65374"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for dair-ai/emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/dair-ai/emotion\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = data['train']\n",
        "validation_data = data['validation']\n",
        "test_data = data['test']\n",
        "\n",
        "# Tokenize data\n",
        "def tokenization(data):\n",
        "    return tokenizer(data['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_data = train_data.map(tokenization, batched=True)\n",
        "validation_data = validation_data.map(tokenization, batched=True)\n",
        "test_data = test_data.map(tokenization, batched=True)\n",
        "\n",
        "# Format data\n",
        "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Set batch size and data loader\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "YB2hnGoib6Hd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_model(model, train_dataloader, optimizer, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "            progress_bar.set_postfix(loss=epoch_loss/(total_predictions//batch_size),\n",
        "                                     accuracy=correct_predictions.item()/total_predictions)\n",
        "            logger.info(f\"Epoch {epoch+1}, Batch {total_predictions//batch_size}, Loss: {epoch_loss/(total_predictions//batch_size):.4f}, Accuracy: {correct_predictions.item()/total_predictions:.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_dataloader)\n",
        "        avg_accuracy = correct_predictions.item() / total_predictions\n",
        "        logger.info(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "train_model(model, train_dataloader, optimizer, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N9grwNHbzRC",
        "outputId": "19a0b8da-d046-42c3-8cc3-bb460bbff0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 250/250 [01:24<00:00,  2.95it/s, accuracy=0.745, loss=0.735]\n",
            "Epoch 2/5: 100%|██████████| 250/250 [01:24<00:00,  2.97it/s, accuracy=0.934, loss=0.175]\n",
            "Epoch 3/5: 100%|██████████| 250/250 [01:23<00:00,  2.99it/s, accuracy=0.95, loss=0.116]\n",
            "Epoch 4/5: 100%|██████████| 250/250 [01:23<00:00,  3.00it/s, accuracy=0.956, loss=0.0937]\n",
            "Epoch 5/5:  41%|████      | 102/250 [00:34<00:48,  3.05it/s, accuracy=0.969, loss=0.071]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = correct_predictions.item() / total_predictions\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "def test_model(model, test_dataloader):\n",
        "    print(\"Testing model...\")\n",
        "    test_loss, test_accuracy = evaluate_model(model, test_dataloader)\n",
        "    print(f\"Test - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_model(model, test_dataloader)"
      ],
      "metadata": {
        "id": "eVAM50e8c2o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(input_text):\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenizer(input_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "    # Move tokens to the appropriate device\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "    # Disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        # Get model output\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Get the logits and apply softmax to get probabilities\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # Get the predicted label\n",
        "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Get the confidence score\n",
        "    confidence = torch.max(probabilities).item()\n",
        "\n",
        "    # Map the label to the corresponding emotion\n",
        "    label_to_emotion = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
        "    emotion = label_to_emotion[predicted_label]\n",
        "\n",
        "    return emotion, confidence\n",
        "\n",
        "# Test the function with a custom string\n",
        "test_string = \"i wan't to have sex with you\"\n",
        "predicted_emotion, confidence = predict_emotion(test_string)\n",
        "print(f\"Input Text: {test_string}\")\n",
        "print(f\"Predicted Emotion: {predicted_emotion} (Confidence: {confidence:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjCKoMUfehx5",
        "outputId": "08948703-733f-4124-ee6d-ebb21e749c2b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: i wan't to have sex with you\n",
            "Predicted Emotion: love (Confidence: 0.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGEvploie3fz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}